<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Bayesian Optimal Pricing, Part 1</title>
  <meta property="og:title" content="Bayesian Optimal Pricing, Part 1" />
  <meta name="twitter:title" content="Bayesian Optimal Pricing, Part 1" />
  <meta name="description" content="Pricing is a common problem faced by businesses, and one that can be addressed effectively by Bayesian statistical methods. We&#39;ll step through a simple example and build the background necessary to extend get involved with this approach.
Let&#39;s start with some hypothetical data. A small company has tried a few different price points (say, one week each) and recorded the demand at each price. We&#39;ll abstract away some economic issues in order to focus on the statistical approach.">
  <meta property="og:description" content="Pricing is a common problem faced by businesses, and one that can be addressed effectively by Bayesian statistical methods. We&#39;ll step through a simple example and build the background necessary to extend get involved with this approach.
Let&#39;s start with some hypothetical data. A small company has tried a few different price points (say, one week each) and recorded the demand at each price. We&#39;ll abstract away some economic issues in order to focus on the statistical approach.">
  <meta name="twitter:description" content="Pricing is a common problem faced by businesses, and one that can be addressed effectively by Bayesian statistical methods. We&#39;ll step through a simple example and build the background necessary to …">
  <meta name="author" content="Chad Scherrer"/>
  <link href='https://cscherrer.github.io/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@@ChadScherrer" />
  <meta name="twitter:creator" content="@@ChadScherrer" />
  <meta property="og:url" content="https://cscherrer.github.io/post/max-profit/" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="Chad Scherrer" />

  <meta name="generator" content="Hugo 0.41" />
  <link rel="canonical" href="https://cscherrer.github.io/post/max-profit/" />
  <link rel="alternate" href="https://cscherrer.github.io/index.xml" type="application/rss+xml" title="Chad Scherrer">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cscherrer.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" /><link rel="stylesheet" href="https://cscherrer.github.io/css/syntax.css" /><link rel="stylesheet" href="https://cscherrer.github.io/css/codeblock.css" />

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114923902-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114923902-1');
</script>





<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">



<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

</head>

  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://cscherrer.github.io">Chad Scherrer</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/">Blog</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/tags">Tags</a>
            </li>
          
        

        

        
      </ul>
    </div>

    <div class="avatar-container">
      <div class="avatar-img-border">
        
      </div>
    </div>

  </div>
</nav>




    
  
  
  




  

  <header class="header-section ">
    
    <div class="intro-header no-img">
      
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              <h1>Bayesian Optimal Pricing, Part 1</h1>
                
                
                  <span class="post-meta">
  
  
  <i class="fa fa-calendar-o"></i>&nbsp;Posted on May 6, 2018
  
  
  &nbsp;|&nbsp;
  <i class="fa fa-clock-o"></i> 9 minutes (1794 words)
  
  
</span>


                
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p><em>Pricing is a common problem faced by businesses, and one that can be addressed effectively by Bayesian statistical methods.  We'll step through a simple example and build the background necessary to extend get involved with this approach.</em></p>

<p>Let's start with some hypothetical data. A small company has tried a few different price points (say, one week each) and recorded the demand at each price. We'll abstract away some economic issues in order to focus on the statistical approach. Here's the data:</p>

<p><figure><img src="plots/q0.svg" alt="png"></figure></p>

<p>That middle data point looks like a posisble outlier; we'll come back to that.</p>

<p>For now, let's start with some notation. We'll use <span  class="math">\(P\)</span> and <span  class="math">\(Q\)</span> to denote price and quantity, respectively. A <span  class="math">\(0\)</span> subscript will denote observed data, so we were able to sell <span  class="math">\(Q_0\)</span> units when the price was set to <span  class="math">\(P_0\)</span>.</p>

<h2 id="building-a-model">Building a Model</h2>

<p>I'm not an economist, but resources I've seen describing the relationship between <span  class="math">\(Q\)</span> and <span  class="math">\(P\)</span> are often of the form</p>

<p><span  class="math">\[
Q = a P^c\ .
\]</span></p>

<p>This ignores that <span  class="math">\(Q\)</span> is not deterministic, but rather follows a distribution determined by <span  class="math">\(P\)</span>. For statistical analysis, it makes more sense to write this in terms of a conditional expectation,</p>

<p><span  class="math">\[
\mathbb{E}[Q|P] = a P^c\ .
\]</span></p>

<p>Taking the log of both sides makes this much more familiar:</p>

<p><span  class="math">\[
\log \mathbb{E}[Q|P] = \log a + c \log P\ .
\]</span></p>

<p>The right side is now a linear function of the unknowns <span  class="math">\(\log a\)</span> and <span  class="math">\(c\)</span>, so considering these as parameters gives us a generalized linear model. Conveniently, the reasonable assumption that <span  class="math">\(Q\)</span> is discrete and comes from independent unit purchases leads us to a Poisson distribution, which fits well with the log link.</p>

<p>Econometrically, this linear relationship in log-log space corresponds to <em>constant price elasticity</em>. This constant elasticity is just the parameter <span  class="math">\(c\)</span>, so fitting the model will also give us an estimate of the elasticity.</p>

<p>Expressing this in PyMC3 is straightforward:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m</span><span class="p">:</span>
    <span class="n">loga</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Cauchy</span><span class="p">(</span><span class="s1">&#39;loga&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Cauchy</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">logμ0</span> <span class="o">=</span> <span class="n">loga</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>
    <span class="err">μ</span><span class="mi">0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;μ0&#39;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logμ0</span><span class="p">))</span>
    <span class="n">qval</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">,</span><span class="err">μ</span><span class="mi">0</span><span class="p">,</span><span class="n">observed</span><span class="o">=</span><span class="n">q0</span><span class="p">)</span></code></pre></div>
<p>There are a few things about this worth noting...</p>

<p>First, we used very broad Cauchy priors. For complex models, too many degrees of freedom can lead to convergence and interpretability problems, similarly to the indentifiability problems that sometimes happen in maximum likelihood estimation. For a simple case like this, it should be enough to make a note of it, in case we run into trouble.</p>

<p>Next, we expect price elasticity of demand <span  class="math">\(c\)</span> to be negative, and we may even have a particular range in mind. For example, it could be reasonable to instead go with something like</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">negc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;-c&#39;</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span></code></pre></div>
<p>We'd then have a minor change to express things in terms of <code>negc</code>.</p>

<p>The <code>μ0</code> line may seem wordier than expected. Yes, we could have just used</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="err">μ</span><span class="mi">0</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">p0</span> <span class="o">**</span> <span class="n">c</span></code></pre></div>
<p>Making the linear predictor explicit makes later changes easier, for example if we decided to include variable elasticity. Wrapping the value in <code>Deterministic</code> just means the sampler will save values of <code>μ0</code> for us. This is a convenience at the cost of additional RAM use, so we'd leave it out for a complex model.</p>

<h2 id="model-fitting-and-diagnostics">Model fitting and Diagnostics</h2>

<p>PyMC3 makes it easy to sample from the posterior:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">m</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span></code></pre></div>
<p>If all parameters are continuous (as in our case), the default is the <em>No-U-Turn Sampler</em> (&quot;NUTS&quot;). Let's get a summary of the result.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">pm</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span></code></pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe tbody tr td {
        text-align: right;
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>mc_error</th>
      <th>hpd_2.5</th>
      <th>hpd_97.5</th>
      <th>n_eff</th>
      <th>Rhat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>loga</th>
      <td>9.31</td>
      <td>1.425</td>
      <td>0.057</td>
      <td>6.369</td>
      <td>12.012</td>
      <td>509.0</td>
      <td>1.0024</td>
    </tr>
    <tr>
      <th>c</th>
      <td>-1.58</td>
      <td> 0.392</td>
      <td> 0.015</td>
      <td>-2.325</td>
      <td>-0.781</td>
      <td>511.0</td>
      <td>1.0026</td>
    </tr>
    <tr>
      <th>μ0__0</th>
      <td>51.38</td>
      <td> 5.854</td>
      <td> 0.210</td>
      <td>40.090</td>
      <td>63.167</td>
      <td>660.0</td>
      <td>1.0013</td>
    </tr>
    <tr>
      <th>μ0__1</th>
      <td>40.11</td>
      <td> 3.145</td>
      <td> 0.089</td>
      <td>34.211</td>
      <td>46.619</td>
      <td>1072.0</td>
      <td>1.0017</td>
    </tr>
    <tr>
      <th>μ0__2</th>
      <td>32.47</td>
      <td> 2.517</td>
      <td> 0.066</td>
      <td>27.487</td>
      <td>37.516</td>
      <td>1311.0</td>
      <td>1.0041</td>
    </tr>
    <tr>
      <th>μ0__3</th>
      <td>27.00</td>
      <td> 2.758</td>
      <td> 0.090</td>
      <td>21.963</td>
      <td>32.720</td>
      <td>819.0</td>
      <td>1.0050</td>
    </tr>
    <tr>
      <th>μ0__4</th>
      <td>22.94</td>
      <td> 3.087</td>
      <td> 0.112</td>
      <td>17.327</td>
      <td>29.193</td>
      <td>660.0</td>
      <td>1.0050</td>
    </tr>
  </tbody>
</table>
</div>

<p>This gives us the posterior mean and standard deviation, along with some other useful information:</p>

<ul>
<li><code>mc_error</code> estimates simulation error by breaking the trace into batches, computing the mean of each batch, and then the standard deviation of these means.</li>
<li><code>hpd_*</code> gives <em>highest posterior density</em> intervals. The 2.5 and 97.5 labels are a bit misleading. There are lots of 95% credible intevals, depending on the relative weights of the left and right tails. The 95% HPD interval is the narrowest among these 95% intervals.</li>
<li><code>n_eff</code> gives the <em>effective number of samples</em>. We took 2000 samples, but there are some significant autocorrelations. For example, our samples from the posterior of <span  class="math">\(c\)</span> have about as much information as if we had taken 511 independent samples.</li>
<li><code>Rhat</code> is sometimes called the <em>potential scale reduction factor</em>, and gives us a factor by which the variance might be reduced, if our MCMC chains had been longer. It's computed in terms of the variance between chains vs within each chain. Values near 1 are good.</li>
</ul>

<p>Back to our problem. Results here look pretty good, with the exception of <span  class="math">\(n_\text{eff}\)</span>.</p>

<p>Let's have a closer look what's going on. Here's the joint posterior of the parameters:</p>

<p><figure><img src="plots/uncentered.png" alt="png"></figure></p>

<p>This strong correlation is caused by the data not being centered. All of the <span  class="math">\(\log P_0\)</span> values are positive, so any increase in the slope leads to a decrease in the intercept, and vice versa. If you don't see this immediatley, draw an <span  class="math">\((x,y)\)</span> cloud of points with positive <span  class="math">\(x\)</span>, and compare slope and intercept of some lines passing through the cloud.</p>

<p>The correlation doesn't cause too much trouble for NUTS, but it can be a big problem for some other samplers like Gibbs sampling. This is also a case where fast variational inference approximations tend to drastically underestimate the variance.</p>

<p>Just to note, this doesn't mean Bayesian method can't handle correlations. Rather, it's that the <em>representation</em> of the model can have an impact. There are lots of ways of dealing with this, including explicit parameterization of the correlation, as well as several reparameterization tricks. Let's look at a simple one.</p>

<h2 id="reparameterization">Reparameterization</h2>

<p>The trouble came from off-center <span  class="math">\(\log P_0\)</span> values, so let's try taking that into account in the model. A quick adjustment does the job:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m2</span><span class="p">:</span>
    <span class="err">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Cauchy</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
    <span class="err">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Cauchy</span><span class="p">(</span><span class="s1">&#39;β&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">logμ0</span> <span class="o">=</span> <span class="err">α</span> <span class="o">+</span> <span class="err">β</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="err">μ</span><span class="mi">0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;μ0&#39;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logμ0</span><span class="p">))</span>
    <span class="n">qval</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s1">&#39;q0&#39;</span><span class="p">,</span><span class="err">μ</span><span class="mi">0</span><span class="p">,</span><span class="n">observed</span><span class="o">=</span><span class="n">q0</span><span class="p">)</span></code></pre></div>
<p>Note that we've changed the names of the parameters, to keep things straight. If we wanted to transform between the two parameterizations, we could equate the two <code>logμ0</code> expressions and solve the system of equations (constant terms equal, and <span  class="math">\(\log P_0\)</span> terms equal).</p>

<p>Let's see how it does.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">m2</span><span class="p">:</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">pm</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">t</span><span class="p">)</span></code></pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe tbody tr td {
        text-align: right;
        vertical-align: top;
    }


</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>mc_error</th>
      <th>hpd_2.5</th>
      <th>hpd_97.5</th>
      <th>n_eff</th>
      <th>Rhat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>α</th>
      <td>3.49</td>
      <td>0.080</td>
      <td>0.002</td>
      <td>3.335</td>
      <td>3.640</td>
      <td>1625.0</td>
      <td>1.0015</td>
    </tr>
    <tr>
      <th>β</th>
      <td>-1.73</td>
      <td>0.411</td>
      <td>0.010</td>
      <td>-2.530</td>
      <td>-0.937</td>
      <td>1657.0</td>
      <td>0.9990</td>
    </tr>
    <tr>
      <th>μ0__0</th>
      <td>52.96</td>
      <td>6.208</td>
      <td>0.148</td>
      <td>41.560</td>
      <td>65.273</td>
      <td>1516.0</td>
      <td>1.0002</td>
    </tr>
    <tr>
      <th>μ0__1</th>
      <td>40.43</td>
      <td>3.253</td>
      <td>0.078</td>
      <td>34.483</td>
      <td>46.979</td>
      <td>1346.0</td>
      <td>1.0014</td>
    </tr>
    <tr>
      <th>μ0__2</th>
      <td>32.10</td>
      <td>2.614</td>
      <td>0.061</td>
      <td>26.667</td>
      <td>36.777</td>
      <td>1673.0</td>
      <td>1.0013</td>
    </tr>
    <tr>
      <th>μ0__3</th>
      <td>26.25</td>
      <td>2.831</td>
      <td>0.065</td>
      <td>20.492</td>
      <td>31.379</td>
      <td>1780.0</td>
      <td>1.0003</td>
    </tr>
    <tr>
      <th>μ0__4</th>
      <td>21.97</td>
      <td>3.103</td>
      <td>0.071</td>
      <td>16.201</td>
      <td>28.079</td>
      <td>1758.0</td>
      <td>0.9998</td>
    </tr>
  </tbody>
</table>
</div>

<p>Much better! Note that it's not the parameters that were affected; <span  class="math">\(n_\text{eff}\)</span> has also improved for the <span  class="math">\(\mu_0\)</span> values, which are directly comparable across models.</p>

<p>And of course, the parameter correlations are much better:</p>

<p><figure><img src="plots/max-profit_11_1.png" alt="png"></figure></p>

<p>There are a few other useful tools we can explore. Here's the <em>trace plot</em>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">t</span><span class="p">);</span></code></pre></div>
<p><figure><img src="plots/traceplot.png" alt="png"></figure></p>

<p>Sampling is done using an independent (Markov) chain for each hardware core (by default, though this can be changed). The trace plot gives us a quick visual check that the chains are behaving well. Ideally, the distributions across chains should look very similar (left column) and have very little autocorrelation (right column). Note that since <span  class="math">\(\mu_0\)</span> is a vector, the plots in the bottom row superimpose its five components.</p>

<p>Sometimes we need a more compact form, especially if there are lots of parameters. In this case the <em>forest plot</em> is helpful.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">pm</span><span class="o">.</span><span class="n">forestplot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">varnames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">,</span><span class="s1">&#39;β&#39;</span><span class="p">])</span></code></pre></div>
<p><figure><img src="plots/forestplot.png" alt="png"></figure></p>

<h2 id="posterior-predictive-checks">Posterior Predictive Checks</h2>

<p>The trace we found is a sample from the joint posterior distribution of the parameters. Instead of a point estimate, we have a sample of &quot;possible worlds&quot; that represent reality. To perform inference, we query each and aggregate the results. So, for example...</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">55</span><span class="p">)</span>
<span class="err">μ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="err">α</span> <span class="o">+</span> <span class="n">t</span><span class="o">.</span><span class="err">β</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="err">μ</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span><span class="n">q0</span><span class="p">,</span><span class="s1">&#39;o&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">);</span></code></pre></div>
<p>To be concise, I've left out some simple code like axis labels, etc. Here's the result.</p>

<p><figure><img src="plots/qhat.png" alt="png"></figure></p>

<p>Each &quot;hair&quot; represents <span  class="math">\(\mathbb{E}[Q|P]\)</span> according to one sample from the posterior. On introducing the data I suggested the point <span  class="math">\(P_0=40\)</span> may be an outlier. At first glance, the above plot might seem to confirm this. But remember, this is not a conditional distribution of <span  class="math">\((Q|P)\)</span>, but of its <em>expectation</em>.</p>

<p>The following boxplot makes this point more explicitly:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">q0</span><span class="p">,</span><span class="n">mu</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="s1">&#39;μ0&#39;</span><span class="p">]));</span></code></pre></div>
<p><figure><img src="plots/q0ppc.png" alt="png"></figure></p>

<p>Think of this as asking each possible world, &quot;In your reality, what's the probability of seeing a smaller <span  class="math">\(Q\)</span> value than this?&quot; The responses are then aggregated by price. For <span  class="math">\(P_0=40\)</span>, about half the results are above 0.1. The point is a bit unusual, but hardly an outlier.</p>

<p>This shouldn't be considered a recipe for <em>the</em> way to do posterior predictive checks. There are countless possibilities, and the right approach depends on the problem at hand.</p>

<p>For example, another very common approach is to use the posterior parameter samples to generate a <em>replicated response</em> <span  class="math">\(Q_0^\text{rep}\)</span>, and then comparing this to the observed response <span  class="math">\(Q_0\)</span>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">q0</span>  <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="s1">&#39;μ0&#39;</span><span class="p">]),</span> <span class="mi">0</span><span class="p">)</span></code></pre></div>
<p>which yields</p>

<pre><code>array([ 0.5055,  0.7265,  0.1055,  0.451 ,  0.7   ])
</code></pre>

<p>These are sometimes called <em>Bayesian <span  class="math">\(p\)</span>-values</em>. For out-of-sample data, results for a well-calibrated model should follow a uniform distribution.</p>

<h2 id="optimization">Optimization</h2>

<p>We're almost there! Suppose we have a per-unit cost of $20. Then we can calculate the expected profit for each realization:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">k</span> <span class="o">=</span> <span class="mi">20</span>
<span class="err">π</span> <span class="o">=</span> <span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="err">μ</span></code></pre></div>
<p>With a few array indexing tricks, we can then calculate the price that maximizes the overall expected profit (integrating over all parameters):</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="err">π</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="err">π</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;$\mathbb{E}[\pi|P]$&#34;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">p</span><span class="p">,(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="err">π</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="err">π</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">,(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="err">π</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="err">π</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="err">π</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="err">π</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;$\mathbb{E}[\pi|P]\ \pm$1 sd&#34;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="err">π</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="err">π</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">pmax</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="err">π</span><span class="p">,</span><span class="mi">1</span><span class="p">))]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">pmax</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">900</span><span class="p">,</span><span class="n">colors</span><span class="o">=</span><span class="s1">&#39;C0&#39;</span><span class="p">,</span><span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;argmax$_P\ \mathbb{E}[\pi|P]$&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span><span class="mi">900</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Price $P$&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Profit $\pi$&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span></code></pre></div>
<p><figure><img src="plots/max_of_means.png" alt="png"></figure></p>

<p>This suggests an optimal price of $49.</p>

<h2 id="caveats">Caveats</h2>

<p>So... are we done? <strong>NO!!!</strong> There are some lingering issues that we'll address next time. For example, is expected profit the right thing to maximize? In particular, our estimate is well above the best worst-case scenario (highest unshaded point below the curves). Should we be concerned about this?</p>

<p>Also, the mean is very flat near our solution, suggesting the potential for instability in the solution. Can we quantify this, and does it lead us to a different solution?</p>

<p><a href="../max-profit-2">Continue to Part 2</a></p>


        

        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://cscherrer.github.io/post/bias-variance/" data-toggle="tooltip" data-placement="top" title="The Bias-Variance Decomposition">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="https://cscherrer.github.io/post/max-profit-2/" data-toggle="tooltip" data-placement="top" title="Bayesian Optimal Pricing, Part 2">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      
        
        
      

    </div>
  </div>
</div>

    <footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
                <a href="mailto:chad.scherrer@gmail.com" title="Email me">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://plus.google.com/&#43;chad.scherrer" title="Google&#43;">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-google-plus fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://github.com/cscherrer" title="GitHub">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://twitter.com/@ChadScherrer" title="Twitter">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://reddit.com/u/cscherrer" title="Reddit">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-reddit-alien fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://linkedin.com/in/chadscherrer" title="LinkedIn">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://stackoverflow.com/users/488124/chad-scherrer" title="StackOverflow">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-stack-overflow fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          <li>
            
            <a href="https://cscherrer.github.io/index.xml" title="RSS">
            
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              <a href="cscherrer.github.io">Chad Scherrer</a>
            
          

          &nbsp;&bull;&nbsp;
          2018

          
            &nbsp;&bull;&nbsp;
            <a href="https://cscherrer.github.io">Chad Scherrer</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="http://gohugo.io">Hugo v0.41</a> powered &nbsp;&bull;&nbsp; Theme by <a href="http://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a> adapted to <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a>
          
        </p>
      </div>
    </div>
  </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha384-dq1/gEHSxPZQ7DdrM82ID4YVol9BYyU7GbWlIwnwyPzotpoc57wDw/guX8EaYGPx" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="https://cscherrer.github.io/js/main.js"></script><script> renderMathInElement(document.body); </script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script>
<script src="https://cscherrer.github.io/js/load-photoswipe.js"></script>






  </body>
</html>

