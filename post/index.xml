<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Chad Scherrer</title>
    <link>https://cscherrer.github.io/post/</link>
    <description>Recent content in Posts on Chad Scherrer</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>chad.scherrer@gmail.com (Chad Scherrer)</managingEditor>
    <webMaster>chad.scherrer@gmail.com (Chad Scherrer)</webMaster>
    <lastBuildDate>Sat, 14 Sep 2019 16:21:50 -0700</lastBuildDate>
    
	<atom:link href="https://cscherrer.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Fast and Flexible Probabilistic Programming with Soss.jl</title>
      <link>https://cscherrer.github.io/post/fast-flexible-probprog/</link>
      <pubDate>Sat, 14 Sep 2019 16:21:50 -0700</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/fast-flexible-probprog/</guid>
      <description>A few months ago, Colin Carroll posted A Tour of Probabilistic Programming Language APIs, where he compared the APIs of a variety of probabilistic programming languages (PPLs) using this model:
\[ \begin{aligned} p ( \mathbf { w } ) &amp; \sim \mathcal { N } \left( \mathbf { 0 } , I _ { 5 } \right) \\ p ( \mathbf { y } | X , \mathbf { w } ) &amp; \sim \mathcal { N } \left( X \mathbf { w } , 0.</description>
    </item>
    
    <item>
      <title>Variational Importance Sampling</title>
      <link>https://cscherrer.github.io/post/variational-importance-sampling/</link>
      <pubDate>Sat, 29 Jun 2019 18:53:45 -0700</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/variational-importance-sampling/</guid>
      <description>Lots of distributions are easy to evaluate (the density), but hard to sample. So when we need to sample such a distribution, we need to use some tricks. We&#39;ll see connections between two of these: importance sampling and variational inference, and see a way to use them together for fast inference.
Importance sampling Importance sampling aims to make it easy to compute expected values. Say we have a distribution \(p\), and we&#39;d like to compute the average of some function \(f\) of the distribution (or equivalently, the expected value of a &amp;quot;push-forward along \(f\)&amp;quot;).</description>
    </item>
    
    <item>
      <title>Confusion Confusion</title>
      <link>https://cscherrer.github.io/post/confusion-confusion/</link>
      <pubDate>Sat, 23 Feb 2019 11:01:39 -0800</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/confusion-confusion/</guid>
      <description>Harder Than it Needs to Be Say you&#39;ve just fit a (two-class) machine learning classifier, and you&#39;d like to judge how it&#39;s doing. This starts out simple: Reality is yes or no, and you predict yes or no. Your model will make some mistakes, which you&#39;d like to characterize.
So you go to Wikipedia, and see this: 
There&#39;s a lot of &amp;quot;divide this sum by that sum&amp;quot;, without much connection to why we&#39;re doing that, or how to interpret the result.</description>
    </item>
    
    <item>
      <title>Soss.jl: Design Plans for Spring 2019</title>
      <link>https://cscherrer.github.io/post/soss-update/</link>
      <pubDate>Sun, 27 Jan 2019 07:35:51 -0800</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/soss-update/</guid>
      <description>If you&#39;ve followed my work recently, you&#39;ve probably heard of my probabilistic programming system Soss.jl. I recently had the pleasure of presenting these ideas at PyData Miami:
[N.B. Above is supposed to be an embedded copy of my slides from PyData Miami. I can see it from Chrome, but not Firefox. Very weird. ]
In April I&#39;ll begin another &amp;quot;passion quarter&amp;quot; (essentially a sabbatical) and hope to really push this work forward.</description>
    </item>
    
    <item>
      <title>Julia for Probabilistic Metaprogramming</title>
      <link>https://cscherrer.github.io/post/soss/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/soss/</guid>
      <description>Since around 2010, I&#39;ve been involved with using and developing probabilistic programming languages. So when I learn about new language, one of my first questions is whether it&#39;s a good fit for this kind of development. In this post, I&#39;ll talk a bit about working in this area with Julia, to motivate my Soss project.
Domain-Specific Languages At a high level, a probabilistic programming languages is a kind of domain-specific language, or DSL.</description>
    </item>
    
    <item>
      <title>A Prelude to Pyro</title>
      <link>https://cscherrer.github.io/post/pyro/</link>
      <pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/pyro/</guid>
      <description>Lately I&#39;ve been exploring Pyro, a recent development in probabilistic programming from Uber AI Labs. It&#39;s an exciting development that has a huge potential for large-scale applications.
In any technical writing, it&#39;s common (at least for me) to realize I need to add some introductory material before moving on. In writing about Pyro, this happened quite a bit, to the point that it warranted this post as a kind of warm-up.</description>
    </item>
    
    <item>
      <title>Bayesian Optimal Pricing, Part 2</title>
      <link>https://cscherrer.github.io/post/max-profit-2/</link>
      <pubDate>Sun, 03 Jun 2018 12:05:48 -0700</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/max-profit-2/</guid>
      <description>This is Part 2 in a series on Bayesian optimal pricing. Part 1 is here.
Introduction In Part 1 we used PyMC3 to build a Bayesian model for sales. By the end we had this result:

A common advantage of Bayesian analysis is the understanding it gives us of the distribution of a given result. For example, we very easily analyze a sample from the posterior distribution of profit for a given price.</description>
    </item>
    
    <item>
      <title>Bayesian Optimal Pricing, Part 1</title>
      <link>https://cscherrer.github.io/post/max-profit/</link>
      <pubDate>Sun, 06 May 2018 07:04:24 -0700</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/max-profit/</guid>
      <description>Pricing is a common problem faced by businesses, and one that can be addressed effectively by Bayesian statistical methods. We&#39;ll step through a simple example and build the background necessary to extend get involved with this approach.
Let&#39;s start with some hypothetical data. A small company has tried a few different price points (say, one week each) and recorded the demand at each price. We&#39;ll abstract away some economic issues in order to focus on the statistical approach.</description>
    </item>
    
    <item>
      <title>The Bias-Variance Decomposition</title>
      <link>https://cscherrer.github.io/post/bias-variance/</link>
      <pubDate>Wed, 04 Apr 2018 13:43:57 -0700</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/bias-variance/</guid>
      <description>Say there&#39;s some experiment that generates noisy data. You and I each go through the process independently, and model the results. Would the resulting models be exactly the same?
Well no, of course not. That&#39;s the whole problem with noise. Instead, we&#39;ll usually end up with something like this (for a quadratic fit):

The idea is that we&#39;d like to find an approximation to \(f(x)\), but we can never observe this function directly.</description>
    </item>
    
    <item>
      <title>Bayesian Changepoint Detection with PyMC3</title>
      <link>https://cscherrer.github.io/post/bayesian-changepoint/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 -0800</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/bayesian-changepoint/</guid>
      <description>A client comes to you with this problem:
 The coal company I work for is trying to make mining safer. We made some change around 1900 that seemed to improve things, but the records are all archived. Tracking down such old records can be expensive, and it would help a lot if we could narrow the search. Can you tell us what year we should focus on?
Also, it would really help to know this is a real effect, and not just due to random variability - we don&#39;t want to waste resources digging up the records if there&#39;s not really anything there.</description>
    </item>
    
  </channel>
</rss>