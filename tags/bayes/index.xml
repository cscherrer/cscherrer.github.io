<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bayes on Chad Scherrer</title>
    <link>https://cscherrer.github.io/tags/bayes/</link>
    <description>Recent content in bayes on Chad Scherrer</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>chad.scherrer@gmail.com (Chad Scherrer)</managingEditor>
    <webMaster>chad.scherrer@gmail.com (Chad Scherrer)</webMaster>
    <lastBuildDate>Sat, 29 Jun 2019 18:53:45 -0700</lastBuildDate>
    
	<atom:link href="https://cscherrer.github.io/tags/bayes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Variational Importance Sampling</title>
      <link>https://cscherrer.github.io/post/variational-importance-sampling/</link>
      <pubDate>Sat, 29 Jun 2019 18:53:45 -0700</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/variational-importance-sampling/</guid>
      <description>Lots of distributions are easy to evaluate (the density), but hard to sample. So when we need to sample such a distribution, we need to use some tricks. We&#39;ll see connections between two of these: importance sampling and variational inference, and see a way to use them together for fast inference.
Importance sampling Importance sampling aims to make it easy to compute expected values. Say we have a distribution \(p\), and we&#39;d like to compute the average of some function \(f\) of the distribution (or equivalently, the expected value of a &amp;quot;push-forward along \(f\)&amp;quot;).</description>
    </item>
    
    <item>
      <title>Soss.jl: Design Plans for Spring 2019</title>
      <link>https://cscherrer.github.io/post/soss-update/</link>
      <pubDate>Sun, 27 Jan 2019 07:35:51 -0800</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/soss-update/</guid>
      <description>If you&#39;ve followed my work recently, you&#39;ve probably heard of my probabilistic programming system Soss.jl. I recently had the pleasure of presenting these ideas at PyData Miami:
 [N.B. Above is supposed to be an embedded copy of my slides from PyData Miami. I can see it from Chrome, but not Firefox. Very weird. ]
In April I&#39;ll begin another &amp;quot;passion quarter&amp;quot; (essentially a sabbatical) and hope to really push this work forward.</description>
    </item>
    
    <item>
      <title>Julia for Probabilistic Metaprogramming</title>
      <link>https://cscherrer.github.io/post/soss/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/soss/</guid>
      <description>Since around 2010, I&#39;ve been involved with using and developing probabilistic programming languages. So when I learn about new language, one of my first questions is whether it&#39;s a good fit for this kind of development. In this post, I&#39;ll talk a bit about working in this area with Julia, to motivate my Soss project.
Domain-Specific Languages At a high level, a probabilistic programming languages is a kind of domain-specific language, or DSL.</description>
    </item>
    
    <item>
      <title>A Prelude to Pyro</title>
      <link>https://cscherrer.github.io/post/pyro/</link>
      <pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/pyro/</guid>
      <description>Lately I&#39;ve been exploring Pyro, a recent development in probabilistic programming from Uber AI Labs. It&#39;s an exciting development that has a huge potential for large-scale applications.
In any technical writing, it&#39;s common (at least for me) to realize I need to add some introductory material before moving on. In writing about Pyro, this happened quite a bit, to the point that it warranted this post as a kind of warm-up.</description>
    </item>
    
    <item>
      <title>Bayesian Optimal Pricing, Part 2</title>
      <link>https://cscherrer.github.io/post/max-profit-2/</link>
      <pubDate>Sun, 03 Jun 2018 12:05:48 -0700</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/max-profit-2/</guid>
      <description>This is Part 2 in a series on Bayesian optimal pricing. Part 1 is here.
Introduction In Part 1 we used PyMC3 to build a Bayesian model for sales. By the end we had this result:

A common advantage of Bayesian analysis is the understanding it gives us of the distribution of a given result. For example, we very easily analyze a sample from the posterior distribution of profit for a given price.</description>
    </item>
    
    <item>
      <title>Bayesian Optimal Pricing, Part 1</title>
      <link>https://cscherrer.github.io/post/max-profit/</link>
      <pubDate>Sun, 06 May 2018 07:04:24 -0700</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/max-profit/</guid>
      <description>Pricing is a common problem faced by businesses, and one that can be addressed effectively by Bayesian statistical methods. We&#39;ll step through a simple example and build the background necessary to extend get involved with this approach.
Let&#39;s start with some hypothetical data. A small company has tried a few different price points (say, one week each) and recorded the demand at each price. We&#39;ll abstract away some economic issues in order to focus on the statistical approach.</description>
    </item>
    
    <item>
      <title>Bayesian Changepoint Detection with PyMC3</title>
      <link>https://cscherrer.github.io/post/bayesian-changepoint/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 -0800</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/bayesian-changepoint/</guid>
      <description>A client comes to you with this problem:
 The coal company I work for is trying to make mining safer. We made some change around 1900 that seemed to improve things, but the records are all archived. Tracking down such old records can be expensive, and it would help a lot if we could narrow the search. Can you tell us what year we should focus on?
Also, it would really help to know this is a real effect, and not just due to random variability - we don&#39;t want to waste resources digging up the records if there&#39;s not really anything there.</description>
    </item>
    
  </channel>
</rss>