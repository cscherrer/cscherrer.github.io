<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Soss on Chad Scherrer</title>
    <link>https://cscherrer.github.io/tags/soss/</link>
    <description>Recent content in Soss on Chad Scherrer</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>chad.scherrer@gmail.com (Chad Scherrer)</managingEditor>
    <webMaster>chad.scherrer@gmail.com (Chad Scherrer)</webMaster>
    <lastBuildDate>Sat, 29 Jun 2019 18:53:45 -0700</lastBuildDate>
    
	<atom:link href="https://cscherrer.github.io/tags/soss/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Variational Importance Sampling</title>
      <link>https://cscherrer.github.io/post/variational-importance-sampling/</link>
      <pubDate>Sat, 29 Jun 2019 18:53:45 -0700</pubDate>
      <author>chad.scherrer@gmail.com (Chad Scherrer)</author>
      <guid>https://cscherrer.github.io/post/variational-importance-sampling/</guid>
      <description>Lots of distributions are easy to evaluate (the density), but hard to sample. So when we need to sample such a distribution, we need to use some tricks. We&#39;ll see connections between two of these: importance sampling and variational inference, and see a way to use them together for fast inference.
Importance sampling Importance sampling aims to make it easy to compute expected values. Say we have a distribution \(p\), and we&#39;d like to compute the average of some function \(f\) of the distribution (or equivalently, the expected value of a &amp;quot;push-forward along \(f\)&amp;quot;).</description>
    </item>
    
  </channel>
</rss>